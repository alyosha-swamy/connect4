{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import concurrent.futures\n",
    "from torch import optim\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import env\n",
    "import network\n",
    "import player\n",
    "\n",
    "BOARD_XSIZE=7\n",
    "BOARD_YSIZE=6\n",
    "DIMS=(BOARD_YSIZE,BOARD_XSIZE)\n",
    "\n",
    "EPISODES_PER_AGENT = 50\n",
    "TRAIN_EPOCHS = 500000\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "MAKE_OPPONENT_INTERVAL = 10000\n",
    "SUMMARY_STATS_INTERVAL = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# create result directory\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    device = cuda\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: restore neural net parameters\n",
    "\n",
    "actor = network.Actor(BOARD_XSIZE, BOARD_YSIZE).to(device)\n",
    "critic = network.Critic(BOARD_XSIZE, BOARD_YSIZE).to(device)\n",
    "\n",
    "def detect_nans(model, input, output):\n",
    "    if output.isnan().any():\n",
    "        raise ValueError(\"NaN found in activations pass!\")\n",
    "\n",
    "actor.register_forward_hook(detect_nans)\n",
    "\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=network.ACTOR_LR)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "# Get Writer\n",
    "writer = SummaryWriter(log_dir=SUMMARY_DIR)\n",
    "\n",
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_buf:list[float] = []\n",
    "\n",
    "opponent_pool:list[player.Player] = [\n",
    "    player.MinimaxPlayer(env.PLAYER2, 2, 0.5),\n",
    "    player.MinimaxPlayer(env.PLAYER2, 2, 0.3),\n",
    "]\n",
    "\n",
    "rewards_vs: dict[str, list[float]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(actor:player.ActorPlayer, opponent: player.Player, actor_turn:bool) -> tuple[\n",
    "    list[env.Observation],\n",
    "    list[env.Action],\n",
    "    list[np.ndarray],\n",
    "    list[env.Reward],\n",
    "    list[env.Advantage],\n",
    "    list[env.Reward],\n",
    "    player.Player,\n",
    "]:\n",
    "    e = env.Env(DIMS)\n",
    "\n",
    "    s_t:list[env.Observation] = []\n",
    "    a_t:list[env.Action] = []\n",
    "    p_t:list[np.ndarray] = []\n",
    "    r_t:list[env.Reward] = []\n",
    "    # play the game\n",
    "    while not e.game_over():\n",
    "        if actor_turn:\n",
    "            obs, action_probs, chosen_action, reward = actor.play(e)\n",
    "            s_t += [obs]\n",
    "            p_t += [action_probs]\n",
    "            a_t += [chosen_action]\n",
    "            r_t += [reward]\n",
    "        else:\n",
    "            opponent.play(e)\n",
    "\n",
    "        # flip turn\n",
    "        actor_turn = not actor_turn\n",
    "\n",
    "    # compute advantage and value\n",
    "    d_t = network.compute_advantage(actor.critic, s_t, r_t)\n",
    "    v_t = network.compute_value(r_t)\n",
    "\n",
    "    return s_t, a_t, p_t, r_t, d_t, v_t, opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     future \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39msubmit(play, actor_player,opponent_player, actor_turn)\n\u001b[1;32m     21\u001b[0m     futures\u001b[39m.\u001b[39mappend(future)\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mas_completed(futures):\n\u001b[1;32m     25\u001b[0m     s_t, a_t, p_t, r_t, d_t, v_t, opp \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39mresult()\n\u001b[1;32m     27\u001b[0m     \u001b[39m# now update the minibatch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m wait_timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m (of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) futures unfinished\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[39mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m waiter\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait(wait_timeout)\n\u001b[1;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m waiter\u001b[39m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for _ in range(TRAIN_EPOCHS):\n",
    "        s_batch:list[env.Observation] = []\n",
    "        a_batch:list[env.Action] = []\n",
    "        p_batch:list[np.ndarray] = []\n",
    "        d_batch:list[env.Advantage] = []\n",
    "        v_batch:list[env.Value] = []\n",
    "        \n",
    "        # create actor player\n",
    "        actor_player = player.ActorPlayer(actor, critic, step, env.PLAYER1)\n",
    "        \n",
    "        futures = []\n",
    "        for _ in range(EPISODES_PER_AGENT):\n",
    "            # pick a random opponent\n",
    "            opponent_player = opponent_pool[np.random.randint(len(opponent_pool))]\n",
    "\n",
    "            actor_turn = np.random.randint(2) == 0\n",
    "\n",
    "            # play the game\n",
    "            future = executor.submit(play, actor_player,opponent_player, actor_turn)\n",
    "            futures.append(future)\n",
    "\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            s_t, a_t, p_t, r_t, d_t, v_t, opp = future.result()\n",
    "\n",
    "            # now update the minibatch\n",
    "            s_batch += s_t\n",
    "            a_batch += a_t\n",
    "            p_batch += p_t\n",
    "            d_batch += d_t\n",
    "            v_batch += v_t\n",
    "\n",
    "            # statistics\n",
    "            opp_name = opp.name()\n",
    "            if opp_name in rewards_vs:\n",
    "                rewards_vs[opp_name].append(float(v_t[-1]))\n",
    "            else:\n",
    "                rewards_vs[opp_name] = [float(v_t[-1])]\n",
    "\n",
    "        actor_losses, critic_losses = network.train_ppo(\n",
    "            actor,\n",
    "            critic,\n",
    "            actor_optimizer,\n",
    "            critic_optimizer,\n",
    "            s_batch,\n",
    "            a_batch,\n",
    "            p_batch,\n",
    "            d_batch,\n",
    "            v_batch\n",
    "        )\n",
    "\n",
    "        for actor_loss, critic_loss in zip(actor_losses, critic_losses):\n",
    "            writer.add_scalar('actor_loss', actor_loss, step)\n",
    "            writer.add_scalar('critic_loss', critic_loss, step)\n",
    "\n",
    "            if step % SUMMARY_STATS_INTERVAL == 0:\n",
    "                for opponent_name, rewards in rewards_vs.items():\n",
    "                    if len(rewards) > 50:\n",
    "                        avg_reward = np.array(rewards).mean()\n",
    "                        writer.add_scalar(f'reward_against_{opponent_name}', avg_reward, step)\n",
    "                        rewards_vs[opponent_name] = []\n",
    "\n",
    "            if step % MAKE_OPPONENT_INTERVAL == 0:\n",
    "                # create a new opponent\n",
    "                frozen_actor = copy.deepcopy(actor)\n",
    "                frozen_actor.eval()\n",
    "                frozen_actor.to(device)\n",
    "                frozen_critic = copy.deepcopy(critic)\n",
    "                frozen_critic.eval()\n",
    "                frozen_critic.to(device)\n",
    "                opponent_pool.append(player.ActorPlayer(frozen_actor, frozen_critic, step, env.PLAYER2))\n",
    "\n",
    "            if step % MODEL_SAVE_INTERVAL == 0:\n",
    "                # Save the neural net parameters to disk.\n",
    "                torch.save(actor.state_dict(), f\"{SUMMARY_DIR}/nn_model_ep_{step}_actor.ckpt\")\n",
    "                torch.save(critic.state_dict(), f\"{SUMMARY_DIR}/nn_model_ep_{step}_critic.ckpt\")\n",
    "            \n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.load_state_dict(torch.load('./summary/nn_model_ep_500_actor.ckpt'))\n",
    "#critic.load_state_dict(torch.load('./summary/nn_model_ep_1500_critic.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_optimizer.param_groups[0]['lr'] = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer.param_groups[0]['lr'] = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True]\n",
      "              \n",
      "              \n",
      "              \n",
      "  #       O   \n",
      "  #       O   \n",
      "  #       O   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "tensor([0.0503, 0.1545, 0.1519, 0.1937, 0.1168, 0.1810, 0.1518],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0.4670, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "e = env.Env(DIMS)\n",
    "\n",
    "e.step(1, 1)\n",
    "e.step(1, 1)\n",
    "e.step(1, 1)\n",
    "\n",
    "e.step(5, 2)\n",
    "e.step(5, 2)\n",
    "e.step(5, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "o = e.observe(1)\n",
    "print(e.legal_mask())\n",
    "env.print_obs(o)\n",
    "print('0 1 2 3 4 5 6 7')\n",
    "print(actor.forward(network.obs_to_tensor(o, device))[0])\n",
    "print(critic.forward(network.obs_to_tensor(o, device))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_value 0.0\n",
      "pred_value 0.26631778478622437\n",
      "actor_probs [0.10306242 0.09808297 0.15853685 0.19781026 0.17623293 0.15890226\n",
      " 0.10737228]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.2533795237541199\n",
      "actor_probs [0.2095273  0.07570179 0.1435569  0.17057098 0.17664906 0.12526788\n",
      " 0.09872611]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "O         #   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.23122821748256683\n",
      "actor_probs [0.4241949  0.05948414 0.10888573 0.14591698 0.10502104 0.08877166\n",
      " 0.06772555]\n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "O             \n",
      "O   #     #   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.2902565002441406\n",
      "actor_probs [0.09175384 0.10523602 0.16459481 0.19411714 0.17112367 0.16101615\n",
      " 0.1121584 ]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.30991435050964355\n",
      "actor_probs [0.11413748 0.09133808 0.1684686  0.20430468 0.16439074 0.13830514\n",
      " 0.11905523]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "    O     #   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.29648029804229736\n",
      "actor_probs [0.11911514 0.08897132 0.16343954 0.20495708 0.16814458 0.13889942\n",
      " 0.11647291]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "O # O     #   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.2671660780906677\n",
      "actor_probs [0.11008973 0.09230784 0.15989724 0.21031684 0.15978529 0.14574935\n",
      " 0.12185367]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "O #           \n",
      "O # O     #   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.2637495696544647\n",
      "actor_probs [0.24226815 0.09964284 0.13453299 0.20501757 0.11085119 0.09102544\n",
      " 0.11666188]\n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "O #       #   \n",
      "O # O     #   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.23298527300357819\n",
      "actor_probs [0.08953246 0.10610654 0.16435112 0.19942147 0.1749738  0.15936947\n",
      " 0.10624507]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "      O       \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.33911362290382385\n",
      "actor_probs [0.06852152 0.14014985 0.16664912 0.25224748 0.12794982 0.13927811\n",
      " 0.10520406]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "      #       \n",
      "      O   O   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.33381718397140503\n",
      "actor_probs [0.05843816 0.14802848 0.16168137 0.31264937 0.10168602 0.12847507\n",
      " 0.08904158]\n",
      "              \n",
      "              \n",
      "              \n",
      "      O       \n",
      "      #       \n",
      "  #   O   O   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.3591756224632263\n",
      "actor_probs [0.03600939 0.10903259 0.08363574 0.12771602 0.529078   0.04157378\n",
      " 0.07295451]\n",
      "              \n",
      "              \n",
      "      #       \n",
      "      O       \n",
      "      #       \n",
      "O #   O   O   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.3645649552345276\n",
      "actor_probs [6.6709723e-03 1.5192599e-02 2.5046787e-03 8.2295649e-03 9.5862454e-01\n",
      " 6.4326939e-04 8.1344135e-03]\n",
      "              \n",
      "              \n",
      "      #       \n",
      "      O       \n",
      "O     #   #   \n",
      "O #   O   O   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.33018600940704346\n",
      "actor_probs [0.03858607 0.04626768 0.02316008 0.02349201 0.8441324  0.00214963\n",
      " 0.0222122 ]\n",
      "              \n",
      "              \n",
      "      #       \n",
      "O     O       \n",
      "O     #   #   \n",
      "O #   O # O   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.42490464448928833\n",
      "actor_probs [2.3792224e-04 6.4247492e-04 9.9837649e-01 2.8768491e-05 9.2375769e-05\n",
      " 1.7950435e-05 6.0398236e-04]\n",
      "              \n",
      "              \n",
      "      #       \n",
      "O     O O     \n",
      "O     # # #   \n",
      "O #   O # O   \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.26631778478622437\n",
      "actor_probs [0.10306242 0.09808297 0.15853685 0.19781026 0.17623293 0.15890226\n",
      " 0.10737228]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.2801194489002228\n",
      "actor_probs [0.08069748 0.10123254 0.16552307 0.19485919 0.17621219 0.16989285\n",
      " 0.11158266]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "#             \n",
      "O O           \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.21163277328014374\n",
      "actor_probs [0.09549493 0.12546335 0.16739994 0.21005505 0.12958209 0.15060657\n",
      " 0.12139802]\n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "# #           \n",
      "O O           \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.22254718840122223\n",
      "actor_probs [0.06751389 0.14958076 0.17938447 0.19968268 0.10574576 0.13825533\n",
      " 0.15983716]\n",
      "              \n",
      "              \n",
      "O             \n",
      "O #           \n",
      "# #           \n",
      "O O           \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.2949126958847046\n",
      "actor_probs [0.04227473 0.13519943 0.19395186 0.22727223 0.05631004 0.19211105\n",
      " 0.15288064]\n",
      "              \n",
      "              \n",
      "O             \n",
      "O #           \n",
      "# #     O     \n",
      "O O     #     \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.0\n",
      "pred_value 0.39713653922080994\n",
      "actor_probs [0.0068324  0.06510064 0.66233313 0.03889999 0.01353526 0.12483247\n",
      " 0.08846616]\n",
      "              \n",
      "O             \n",
      "O             \n",
      "O #           \n",
      "# #     O     \n",
      "O O   # #     \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.47829690000000014\n",
      "pred_value 0.2902565002441406\n",
      "actor_probs [0.09175384 0.10523602 0.16459481 0.19411714 0.17112367 0.16101615\n",
      " 0.1121584 ]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.5314410000000002\n",
      "pred_value 0.3157567083835602\n",
      "actor_probs [0.08906432 0.10663971 0.1642799  0.1928332  0.16998622 0.16296469\n",
      " 0.11423193]\n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "              \n",
      "#           O \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.5904900000000002\n",
      "pred_value 0.22897164523601532\n",
      "actor_probs [0.15545776 0.11461077 0.16636221 0.18385294 0.13033596 0.13909648\n",
      " 0.11028382]\n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "#             \n",
      "#           O \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.6561000000000001\n",
      "pred_value 0.34041646122932434\n",
      "actor_probs [0.13538456 0.13500804 0.17360169 0.15700813 0.10299623 0.17601915\n",
      " 0.11998216]\n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "#             \n",
      "#     #   O O \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.7290000000000001\n",
      "pred_value 0.28848251700401306\n",
      "actor_probs [0.09841922 0.16465795 0.19075805 0.16966946 0.08939008 0.14927988\n",
      " 0.13782535]\n",
      "              \n",
      "              \n",
      "              \n",
      "O             \n",
      "#     O   #   \n",
      "#     #   O O \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.81\n",
      "pred_value 0.3069721758365631\n",
      "actor_probs [0.105984   0.1801414  0.19632082 0.19220267 0.08464942 0.09906609\n",
      " 0.14163558]\n",
      "              \n",
      "              \n",
      "              \n",
      "O         #   \n",
      "#     O   #   \n",
      "#     # O O O \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 0.9\n",
      "pred_value 0.35693359375\n",
      "actor_probs [0.07648996 0.25402147 0.1459347  0.15198965 0.21088353 0.02730861\n",
      " 0.1333721 ]\n",
      "              \n",
      "              \n",
      "O             \n",
      "O     #   #   \n",
      "#     O   #   \n",
      "#     # O O O \n",
      "\n",
      "0 1 2 3 4 5 6 7\n",
      "real_value 1.0\n",
      "pred_value 0.41492414474487305\n",
      "actor_probs [0.05092614 0.17258082 0.3549207  0.12885228 0.16623004 0.01517949\n",
      " 0.11131049]\n",
      "              \n",
      "              \n",
      "O             \n",
      "O     #   #   \n",
      "#     O   # O \n",
      "# #   # O O O \n",
      "\n",
      "0 1 2 3 4 5 6 7\n"
     ]
    }
   ],
   "source": [
    "s_tensor = network.obs_batch_to_tensor(s_batch, device)\n",
    "critic_guesses = critic.forward(s_tensor).to(cpu).detach().numpy()\n",
    "actor_guesses = actor.forward(s_tensor).to(cpu).detach().numpy()\n",
    "for v, obs, critic_guess, actor_guess in zip(v_batch, s_batch, critic_guesses, actor_guesses):\n",
    "    print(\"real_value\", v)\n",
    "    print(\"pred_value\", float(critic_guess))\n",
    "    print(\"actor_probs\", np.array(actor_guess))\n",
    "    env.print_obs(obs)\n",
    "    print('0 1 2 3 4 5 6 7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
